{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"datasets/plain.txt\",sep='\\n',skiprows=2,header=None)\n",
    "text.head()\n",
    "text.rename(columns={0:'txt'},inplace=True)\n",
    "text2 = pd.read_csv(\"datasets/sql_querys.txt\",sep='\\n',header=None)\n",
    "s1=list(text['txt'])\n",
    "s2=list(text2[0])\n",
    "s1=[i.lower() for i in s1]\n",
    "s2=[i.lower() for i in s2]\n",
    "y1=[0 for i in range(len(s1))]\n",
    "y2=[1 for i in range(len(s2))]\n",
    "y=y1+y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lem=WordNetLemmatizer()\n",
    "\n",
    "new_s1=[]\n",
    "stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    "for i in s1:\n",
    "    temp=i.split(' ')\n",
    "    temp1=[]\n",
    "    for j in temp:\n",
    "\n",
    "        j=j.strip(',').strip(r'[\"]').strip(';').strip('“').strip('”').strip(r'[.]+')\n",
    "        j=j.replace(\"’\",'')\n",
    "        if j not in stopwords and j.isalpha():\n",
    "            \n",
    "            j=word_lem.lemmatize(j)\n",
    "            temp1.append(j)\n",
    "\n",
    "    new_s1.append(' '.join(temp1))\n",
    "    \n",
    "new_s2=[]\n",
    "for i in s2:\n",
    "    temp=i.split(' ')\n",
    "    temp1=[]\n",
    "    for j in temp:\n",
    "        if j not in stopwords:\n",
    "            \n",
    "            j=word_lem.lemmatize(j)\n",
    "            temp1.append(j)\n",
    "\n",
    "    new_s2.append(' '.join(temp1))\n",
    "\n",
    "new_s = new_s1 + new_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = Tokenizer(num_words = 100000, oov_token=\"<OOV>\")\n",
    "tokenizer1.fit_on_texts(new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter sql query for check: \"HELLO union ALL select NULL, version()--\"\n",
      "\"hello union all select null, version()--\" - is identified as a malicious query !!!\n"
     ]
    }
   ],
   "source": [
    "entry = input(\"Please enter sql query for check: \")\n",
    "entry = entry.lower()\n",
    "temp = []\n",
    "temp.append(entry)\n",
    "\n",
    "token = tokenizer1.texts_to_sequences(temp)\n",
    "pad = pad_sequences(token, maxlen=150,padding=\"post\")\n",
    "\n",
    "decision = model.predict_proba(pad)[0][0]\n",
    "\n",
    "if(decision < 0.6):\n",
    "    print(f\"{entry} - is not identified as a malicious query\")\n",
    "else:\n",
    "    print(f\"{entry} - is identified as a malicious query !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql_injection",
   "language": "python",
   "name": "sql_injection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
